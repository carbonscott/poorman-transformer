{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb85c04b570>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poorman_transformer.data                 import TokenManager\n",
    "from poorman_transformer.modeling.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"input.txt\"\n",
    "with open(input_file_path, 'r') as fh:\n",
    "    data = fh.read()\n",
    "token_lib = sorted(list(set(data)))\n",
    "token_manager = TokenManager(token_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(''.join(token_lib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(data[0:0+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210497 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "token_lib_size = len(token_lib)\n",
    "embd_size      = 64\n",
    "num_blocks     = 4\n",
    "head_size      = 64 // 4\n",
    "context_length = 32\n",
    "num_heads      = embd_size // head_size\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(token_lib_size, embd_size, context_length, num_blocks, num_heads).to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [[[ USER INPUT ]]]\n",
    "timestamp_prev = \"2023_0822_1233_05\"\n",
    "# epoch          = 240\n",
    "epoch          = 240\n",
    "\n",
    "drc_chkpt = \"chkpts\"\n",
    "fl_chkpt_prev   = None if timestamp_prev is None else f\"{timestamp_prev}.epoch_{epoch}.chkpt\"\n",
    "path_chkpt_prev = None if fl_chkpt_prev is None else os.path.join(drc_chkpt, fl_chkpt_prev)\n",
    "\n",
    "chkpt = torch.load(path_chkpt_prev)\n",
    "\n",
    "# Load weights...\n",
    "model.module.load_state_dict(chkpt['model_state_dict']) if hasattr(model, 'module') else \\\n",
    "        model.load_state_dict(chkpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_one_char(s, delay=0.05):\n",
    "    import time\n",
    "    for char in s:\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(delay)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb85c04b570>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 7\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___/ Input \\___\n",
      "X-ray Beam is \n",
      "\n",
      "___/ Output \\___\n",
      "X-ray Beam is kindness.\n",
      "\n",
      "BENVOLIO:\n",
      "I will I myself look.\n",
      "\n",
      "Clown:\n",
      "Ho&s murder I\n",
      "the woest been postern bends make you.\n",
      "\n",
      "First Keeper:\n",
      "In lay he is here it up and let me to my fathers,\n",
      "Shall news of the wars of any his feet.\n",
      "3 KING HENRY VI\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Against pocts is, whose that thou art.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Shall ever doth in the more of blood,\n",
      "Forture, palace for 'tis now chief:\n",
      "You have need thou proclame, talk he is a same too, sheeps, went a man,\n",
      "Or cousin, well met, do\n",
      "eye, no.\n",
      "\n",
      "ISABELLA:\n",
      "Good one me\n"
     ]
    }
   ],
   "source": [
    "input = \"X-ray Beam is \"\n",
    "input = input[-context_length:]\n",
    "input_encoded = token_manager.encode(input[-context_length:])\n",
    "input_encoded = torch.tensor(input_encoded)[None,].to(device)\n",
    "\n",
    "print(f\"___/ Input \\___\")\n",
    "print(input)\n",
    "print(\"\")\n",
    "\n",
    "prediction_length = 500\n",
    "for _ in range(prediction_length):\n",
    "    next_token_encoded = model.generate_one(input_encoded[:,-context_length:])\n",
    "    input_encoded = torch.cat([input_encoded, next_token_encoded], dim = 1)\n",
    "\n",
    "output = token_manager.decode(input_encoded.cpu().tolist()[0])\n",
    "print(f\"___/ Output \\___\")\n",
    "# print(f\"{output}\")\n",
    "print_one_char(output, delay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peaknet-1.0",
   "language": "python",
   "name": "peaknet-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
